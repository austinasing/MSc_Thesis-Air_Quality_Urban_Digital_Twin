{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Hourly Regression-Kriging EAQI Model for Sofia, Bulgaria</center></h1>\n",
    "\n",
    "This Jupyter Note Book goes through the methodology of collecting data, processing data, computing the regression coefficients, and generating the final prediction map in a step-by-step process.  \n",
    "Steps 1-4 are run one time in the beginning. Step 5 and 6-? are two scripts that run hourly in the data platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Step 1: Initialize requirements</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "data_folder = r'C:\\Users\\Austin\\Documents\\DATABANK\\Masters\\Thesis\\Spatial_Data\\v5'\n",
    "f_airport_r = data_folder + r'\\prox2_airport_prox.tif'\n",
    "f_airport_v = data_folder + r'\\airport.gpkg'\n",
    "f_boundary_v = data_folder + r'\\boundary.gpkg'\n",
    "f_boundary_r = data_folder + r'\\boundary.tif'\n",
    "f_build_h = data_folder + r'\\build_height.tif'\n",
    "f_bus_stops = data_folder + r'\\bus_stops.gpkg'\n",
    "f_elevation = data_folder + r'\\elevation.tif'\n",
    "f_landuse = data_folder + r'\\landuse_7801.tif'\n",
    "f_major_rd_v = data_folder + r'\\major_rd.gpkg'\n",
    "f_major_rd_r = data_folder + r'\\major_rd.tif'\n",
    "f_major_rd_prox = data_folder + r'\\prox2_mj_road_prox.tif'\n",
    "f_minor_rd = data_folder + r'\\minor_rd.tif'\n",
    "f_ndvi = data_folder + r'\\ndvi.tif'\n",
    "f_parking = data_folder + r'\\parking.tif'\n",
    "f_pop = data_folder + r'\\pop.tif'\n",
    "f_pplant_r = data_folder + r'\\prox2_v5_pplant_prox.tif'\n",
    "f_pplant_v = data_folder + r'\\pplant.gpkg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "# API Collection\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import overpy\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Spatial data processing\n",
    "from pyproj import Transformer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box, Point, LineString, mapping\n",
    "from shapely.affinity import translate\n",
    "from shapely.ops import nearest_points \n",
    "from shapely.strtree import STRtree\n",
    "import rasterio\n",
    "from rasterio import mask\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.transform import rowcol\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "#import fiona\n",
    "# Statistical models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from scipy.stats import pearsonr\n",
    "import cvxpy as cp\n",
    "\n",
    "# Misc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "# Database\n",
    "from sqlalchemy import create_engine, Column, Integer, Float, String, DateTime, ARRAY, inspect\n",
    "from sqlalchemy.sql import text\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import create_engine, Column, Integer, Float, String, DateTime, text\n",
    "from sqlalchemy.orm import Session, sessionmaker\n",
    "from datetime import timedelta\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "import logging\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API credentials\n",
    "username = 'testUser'\n",
    "password = 'test-1234-user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project CRS: LOCAL_CS[\"BGS2005 / CCS2005\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n",
      "LOCAL_CS[\"BGS2005 / CCS2005\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n"
     ]
    }
   ],
   "source": [
    "# Set Coordinate Reference Objects\n",
    "# Defined here due to issues with OGR reading CRS of data\n",
    "\n",
    "epsg4326_wkt = '''GEOGCS[\"WGS 84\",\n",
    "    DATUM[\"WGS_1984\",\n",
    "        SPHEROID[\"WGS 84\",6378137,298.257223563,\n",
    "            AUTHORITY[\"EPSG\",\"7030\"]],\n",
    "        AUTHORITY[\"EPSG\",\"6326\"]],\n",
    "    PRIMEM[\"Greenwich\",0,\n",
    "        AUTHORITY[\"EPSG\",\"8901\"]],\n",
    "    UNIT[\"degree\",0.0174532925199433,\n",
    "        AUTHORITY[\"EPSG\",\"9122\"]],\n",
    "    AUTHORITY[\"EPSG\",\"4326\"]]'''\n",
    "epsg3035_wkt = '''PROJCS[\"ETRS89-extended / LAEA Europe\",\n",
    "    GEOGCS[\"ETRS89\",\n",
    "        DATUM[\"European_Terrestrial_Reference_System_1989\",\n",
    "            SPHEROID[\"GRS 1980\",6378137,298.257222101,\n",
    "                AUTHORITY[\"EPSG\",\"7019\"]],\n",
    "            TOWGS84[0,0,0,0,0,0,0],\n",
    "            AUTHORITY[\"EPSG\",\"6258\"]],\n",
    "        PRIMEM[\"Greenwich\",0,\n",
    "            AUTHORITY[\"EPSG\",\"8901\"]],\n",
    "        UNIT[\"degree\",0.0174532925199433,\n",
    "            AUTHORITY[\"EPSG\",\"9122\"]],\n",
    "        AUTHORITY[\"EPSG\",\"4258\"]],\n",
    "    PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],\n",
    "    PARAMETER[\"latitude_of_center\",52],\n",
    "    PARAMETER[\"longitude_of_center\",10],\n",
    "    PARAMETER[\"false_easting\",4321000],\n",
    "    PARAMETER[\"false_northing\",3210000],\n",
    "    UNIT[\"metre\",1,\n",
    "        AUTHORITY[\"EPSG\",\"9001\"]],\n",
    "    AUTHORITY[\"EPSG\",\"3035\"]]'''\n",
    "\n",
    "crs_4326 = CRS.from_wkt(epsg4326_wkt)\n",
    "crs_3035 = CRS.from_wkt(epsg3035_wkt)\n",
    "\n",
    "# Use local Bulgarian CRS used in building file\n",
    "with rasterio.open(f_boundary_r) as src:\n",
    "    proj_crs = src.crs\n",
    "    print(f'Project CRS: {proj_crs.to_string()}')\n",
    "\n",
    "print(proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Step 2: Set-up PostGreSQL database and tables<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database platform_db already exists\n",
      "Tables created\n"
     ]
    }
   ],
   "source": [
    "# Set up sql database and tables\n",
    " \n",
    "# Database parameters\n",
    "dbname='platform_db'\n",
    "host='localhost'\n",
    "user='postgres'\n",
    "password='postgres'\n",
    "# SQL file to create tables\n",
    "sql_file = 'setup_tables.sql'\n",
    "\n",
    "# Function to set up the database\n",
    "def setup_database(dbname):\n",
    "    # Connect to server\n",
    "    conn = psycopg2.connect(host=host, user=user, password=password)\n",
    "    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT) # Set isolation level needed for db creation\n",
    "    cursor = conn.cursor()\n",
    "    # Create database (check if it exists first)\n",
    "    cursor.execute(\"SELECT 1 FROM pg_database WHERE datname = %s\", (dbname,))\n",
    "    exists = cursor.fetchone()\n",
    "    if not exists:\n",
    "        cursor.execute(f\"CREATE DATABASE {dbname}\")\n",
    "        print(f'Database {dbname} created')\n",
    "    else:\n",
    "        print(f\"Database {dbname} already exists\")\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Function to set up the tables in the db, runs the prepared sql script\n",
    "def setup_tables(sql_file):\n",
    "    # Connect to the new db\n",
    "    conn = psycopg2.connect(dbname=dbname, host=host, user=user, password=password)\n",
    "    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    cursor = conn.cursor()\n",
    "    # Load and execute SQL script\n",
    "    with open(sql_file, 'r') as f:\n",
    "        sql_script = f.read()\n",
    "    try:\n",
    "        cursor.execute(sql_script)\n",
    "        print('Tables created')\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing SQL script: {e}\")\n",
    "        # You might want to log the error or handle it more specifically\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Set up database\n",
    "setup_database(dbname)\n",
    "# Set up tables\n",
    "setup_tables(sql_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Step 3: Collect Historical Sensor Data</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "# Collect all raw historical data from API into df/csv\n",
    "# #visualize key parts\n",
    "# Perform filtering\n",
    "# Save processed data to sensor_data database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station information collected. 39 stations\n"
     ]
    }
   ],
   "source": [
    "# Collect air pollution station information\n",
    "\n",
    "station_url = 'https://citylab.gate-ai.eu/sofiasensors/api/stations/'\n",
    "# Use requests to request the API\n",
    "response = requests.get(station_url, auth=HTTPBasicAuth(username,password))\n",
    "response.raise_for_status()\n",
    "stations_data = response.json()\n",
    "# Save the important information to a pandas dataframe\n",
    "all_stations_df = pd.DataFrame(stations_data, columns=['name', 'longitude', 'latitude', 'operator'])\n",
    "# Clean up the operator names\n",
    "all_stations_df['operator'] = all_stations_df['operator'].replace('GATE Institute', 'GATE')\n",
    "all_stations_df['operator'] = all_stations_df['operator'].replace('Executive environmental agency (ExEA)', 'ExEA')\n",
    "all_stations_df['operator'] = all_stations_df['operator'].replace('Sofia municipality', 'AirThings')\n",
    "\n",
    "# Convert Lat/Lon values into EPSG7801\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:7801\", always_xy=True)\n",
    "def reproject_coords(row):\n",
    "    longitude, latitude = row['longitude'], row['latitude']\n",
    "    x, y = transformer.transform(longitude, latitude)\n",
    "    # Round the coordinates to decimeters\n",
    "    x, y = round(x, ndigits=1), round(y, ndigits=1)\n",
    "    return (x,y)\n",
    "all_stations_df['location'] = all_stations_df.apply(reproject_coords, axis=1)\n",
    "all_stations_df.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "\n",
    "print(f'Station information collected. {len(all_stations_df)} total stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect historical hourly measurement data\n",
    "\n",
    "# Creates an empty dataframe of all of the hours and stations for the year: \n",
    "def create_empty_df(station_df, start_date, end_date):\n",
    "    # Generate hourly timestamps with pandas.daterange\n",
    "    timestamps = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "    # Repeat each timestamp for the amount of stations\n",
    "    timestamps_repeated = np.repeat(timestamps, len(station_df))\n",
    "    # Tile the station names over each timestamp\n",
    "    stations_repeated = np.tile(station_df['name'], len(timestamps))\n",
    "    # Construct the DataFrame with organized entries for each hour and station\n",
    "    empty_df = pd.DataFrame({'time': timestamps_repeated, 'name': stations_repeated})\n",
    "    # Format the 'time' column to 'YYYY-MM-DD HH:00:00' to align with API response format\n",
    "    empty_df['time'] = empty_df['time'].dt.strftime('%Y-%m-%d %h:00:00')\n",
    "    # Add parameter columns as NaN values to station_data\n",
    "    for param in total_params:\n",
    "        empty_df[param] = pd.NA\n",
    "    # Set a multi-index for efficient updating\n",
    "    empty_df.set_index(['time','name'], inplace=True)\n",
    "    print(f'Dataframe for {start_date} to {end_date} created')\n",
    "    return empty_df   \n",
    "\n",
    "# Fills the empty dataframe with measurement values from one year\n",
    "def station_data_collection(station_df, start_date, end_date):\n",
    "    # Create an empty dataframe to fill in with station data\n",
    "    # Hours without measurements will not be returned in API\n",
    "    year_df = create_empty_df(station_df, start_date, end_date)\n",
    "    # Iterate over each station\n",
    "    for _,station in all_stations_df.iterrows():\n",
    "        station_name = station['name']\n",
    "        station_op = station['operator']\n",
    "        # Set collected parameters based on the station operator\n",
    "        # Only using ExEA stations for wind variables due to its overlap with AirThings stations and collection of limited pollutants\n",
    "        if station_op == 'ExEA':\n",
    "            params = total_params[5:]\n",
    "        if station_op in ['AirThings','GATE']:\n",
    "            params =  total_params[:5]\n",
    "        # Iterate over each parameter to collect one in each request\n",
    "        for param in params:\n",
    "            # Reformat space in param string to fit API parameters\n",
    "            param_str = param.replace(' ','%20')\n",
    "            # URL for the 'chart' endpoint which collects serial data the quickest\n",
    "            yearly_url = rf'https://citylab.gate-ai.eu/sofiasensors/api/aggregated/chart/measurements/?station_name={station_name}&parameter_name={param_str}&start_date={start_date}%2019%3A00%3A00&end_date={end_date}%2019%3A00%3A00'\n",
    "            # Try API request\n",
    "            try:\n",
    "                response = requests.get(yearly_url, auth=HTTPBasicAuth(username,password), timeout=30)\n",
    "                data = response.json()\n",
    "                response.raise_for_status() # Raises HTTP error that I can catch and interpret\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Request failed for station {station_name}: {e}\")\n",
    "                continue # If request fails, move on to next parameter\n",
    "            # If the data is empty, move on to the next parameter\n",
    "            if not data:\n",
    "                print(f'No data for {station_name}, {param}')\n",
    "                continue\n",
    "            print(f'{len(data)} measured values for {station_name},{param}')\n",
    "            # Create a df to format the response json with the station name and param\n",
    "            st_param_df = pd.DataFrame([{'time': list(d.keys())[0], \n",
    "                                         'name': station_name, \n",
    "                                         param : list(d.values())[0]} for d in data])\n",
    "            # Set the multi-index for this df to align it with year_df\n",
    "            st_param_df.set_index(['time','name'], inplace=True)\n",
    "            # Update the empty df values with the new station and parameter\n",
    "            year_df.update(st_param_df)\n",
    "        print(f'{station_name} data collected...')\n",
    "    # Reset station_data index to restore name and time columns\n",
    "    year_df.reset_index(inplace=True)\n",
    "\n",
    "    # Save the df to the table\n",
    "\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "# Define parameters to be collected\n",
    "total_params = ['Ozone','Nitrogen dioxide', 'Sulphur dioxide', 'Particulate matter 10', 'Particulate matter 2.5', 'Temperature', 'Pressure', 'Relative humidity', 'Wind direction', 'Wind speed']\n",
    "# Collect from AirThings start point (May 1st 2020)\n",
    "for year in range(2020,2025):\n",
    "    start_date = f'{year}-05-01'\n",
    "    # Set the end date a year after unless it's this year\n",
    "    if year < 2024:\n",
    "        end_date = f'{year+1}-05-01'\n",
    "    else:\n",
    "        end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "    # Run yearly collection function for this particular year\n",
    "    # One year at a time due to memory and HTTP request size constraints\n",
    "    station_data_collection(all_stations_df, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor Data Processing / Filtering\n",
    "\n",
    "# Collect from API and port to own database\n",
    "# Visualize measurements\n",
    "    # Gaps\n",
    "    # Differences between operators (mostly A vs AT)\n",
    "\n",
    "# Decide data start date\n",
    "# Remove stations from visualization\n",
    "# Remove error values: (0, t=5368739, outliers (over whole dataset))\n",
    "# Linear intepolation <=3 hour gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload processed data to sensor_data db\n",
    "# need to do some reformatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Step 4: Calculate Static Independant Variable Values<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate independant variable values \n",
    "# Including the ones with 8 variables with wind direction\n",
    "    # Urban land use, population, road length, bus stop, parking lot\n",
    "    # Could also change variables for powerplant / airport (to a yes no is it in the buffer or not)(this is more logical to me)\n",
    "    # Amounts to 8 non-wind and 8 wind-directioned(64) = 72\n",
    "    # 13 buffers = 936 values for each station (only 208 actual different variables)\n",
    "        # 13 could be overkill though. Anywhere 5-9 is reasonable I think (as long as there is citation)\n",
    "    # These values can be stored in sql, but will just want to be in memory when using\n",
    "        # Can also use persistent volume not with SQL (i'm just reading the data)\n",
    "\n",
    "# Precalculate the indep variable maps \n",
    "    # Think first about how the mapping function will use these (with wind dir). May structure differently than 1 map per wind direction\n",
    "# Store in SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all variable values\n",
    "\n",
    "# Function to create a mask for a raster with a single buffer size\n",
    "def create_mask(raster_info, buffer_size):\n",
    "    pixel_size = raster_info['resolution']\n",
    "    radius_px = int(np.round(buffer_size / pixel_size))\n",
    "    # Size of square to hold the circle\n",
    "    size = 2 * radius_px + 1\n",
    "    center = radius_px # Center of the square is at radius,radius\n",
    "    # Create grid of indices\n",
    "    y,x = np.ogrid[:size, :size]\n",
    "    # Calculate grid of distance from the center\n",
    "    distance_squared = (x - center) ** 2 + (y - center) ** 2\n",
    "    # Mask the distance grid for only values within the radius\n",
    "    return distance_squared <= radius_px ** 2\n",
    "\n",
    "# Function to get the index of a raster for a point\n",
    "def get_cell_indices(point, raster_info): \n",
    "    xmin = raster_info['xmin']\n",
    "    ymax = raster_info['ymax']\n",
    "    resolution = raster_info['resolution']\n",
    "    pointx, pointy = point.x, point.y\n",
    "    j = ((pointx - xmin) / resolution)\n",
    "    i = ((ymax - pointy) / resolution)\n",
    "    return round(i), round(j)\n",
    "\n",
    "# Function which collects cells from the mask\n",
    "def get_buffer_cells(point_index, array, mask):\n",
    "    rows, cols = array.shape\n",
    "    point_r, point_c = point_index\n",
    "    mask_size = mask.shape[0]\n",
    "    radius = mask_size // 2\n",
    "    # Calculate bounds for the slice in the main array\n",
    "    row_start = max(0, point_r - radius)\n",
    "    row_end = min(rows, point_r + radius + 1)\n",
    "    col_start = max(0, point_c - radius)\n",
    "    col_end = min(cols, point_c + radius + 1)\n",
    "    # Bounds for the mask slice to align with array slice\n",
    "    mask_row_start = max(0, radius - point_r)\n",
    "    mask_row_end = mask_size - max(0, (point_r + radius + 1) - rows)\n",
    "    mask_col_start = max(0, radius - point_c)\n",
    "    mask_col_end = mask_size - max(0, (point_c + radius + 1) - cols)\n",
    "    # Extract the slices\n",
    "    array_slice = array[row_start:row_end, col_start:col_end]\n",
    "    mask_slice = mask[mask_row_start:mask_row_end, mask_col_start:mask_col_end]\n",
    "    return array_slice[mask_slice]\n",
    "\n",
    "# Function to calculate the value of a buffer\n",
    "def calculate_buffer_value(point, var, raster_info, mask):\n",
    "    # Convert point to cell coordinate in raster\n",
    "    point_index = get_cell_indices(point, raster_info)\n",
    "    # Extract the cells in 1d array \n",
    "    buffer_cells = get_buffer_cells(point_index, raster_info['array'], mask)\n",
    "    total_cells = buffer_cells.size\n",
    "    # Filter nodata values\n",
    "    nodata_val = raster_info['nodata']\n",
    "    valid_cells = buffer_cells[buffer_cells != nodata_val]\n",
    "    # Calculation depending on variable\n",
    "    # Land use = Amount of land use type within the buffer\n",
    "    if var == 'lu_urban':\n",
    "        value = np.isin(valid_cells, 1).sum()\n",
    "    elif var == 'lu_grass':\n",
    "        value = np.isin(valid_cells, 2).sum()\n",
    "    elif var == 'lu_forest':\n",
    "        value = np.isin(valid_cells, 3).sum()\n",
    "    elif var in ('build_cover', 'build_vol', 'build_std'):\n",
    "        count = np.count_nonzero(valid_cells)\n",
    "        # Building cover = % buildings over buffer size\n",
    "        if var == 'build_cover':\n",
    "            value = count / total_cells\n",
    "        # Building volume = building height * area\n",
    "        elif var == 'build_vol':\n",
    "            value = np.sum(valid_cells) * count * 10\n",
    "        # Building height variation = standard deviation of building heights\n",
    "        elif var == 'build_std':\n",
    "            if len(valid_cells) >= 2: # Can't have deviation between less than 2 data points\n",
    "                value = np.std(valid_cells)\n",
    "            else:\n",
    "                value = 0\n",
    "    # Elevation, population, and ndvi are all average values in the buffer\n",
    "    elif var in ['elevation', 'pop', 'ndvi']:\n",
    "        value = float(np.mean(valid_cells))\n",
    "    # Roads and parking lots: count total valid cells\n",
    "    elif var in ('mj_road_ln', 'mi_road_ln', 'parking'):\n",
    "        value = np.count_nonzero(valid_cells)\n",
    "    return value\n",
    "\n",
    "# Function to calculate the amount of points within vector buffer (for bus stop calculation)\n",
    "def calculate_vector_buffer(point, vector_data, buffer_geom):\n",
    "    # Translate buffer template to the point\n",
    "    point_buffer = translate(buffer_geom, xoff=point.x, yoff=point.y)\n",
    "    # Filter the points within the buffer and return the value\n",
    "    inner_points = vector_data[vector_data.geometry.within(point_buffer)]\n",
    "    return len(inner_points)\n",
    "\n",
    "# Function to do distance calculations for power plant, major roads, and airport\n",
    "def distance_calculations(point, tree, geoms):\n",
    "    # Find nearest linestring in the tree\n",
    "    nearest_index = tree.nearest(point)\n",
    "    nearest_geom = geoms[nearest_index]\n",
    "    # Use nearest points as it is more efficient for bigger dataset like network\n",
    "    p1, p2 = nearest_points(point, nearest_geom)\n",
    "    return p1.distance(p2)\n",
    "\n",
    "# Function to parallel compute the function with input points\n",
    "def parallel_compute(func, input_points):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(func, input_points))\n",
    "    return results\n",
    "\n",
    "# Main calculation function\n",
    "def calculate_points(stations, input_calculation, buffer_size):\n",
    "    # Make sure I'm working with global variable\n",
    "    global computed_variables\n",
    "    # Define variables from input calculation\n",
    "    var = input_calculation['var']\n",
    "    data_file = input_calculation['var_data']\n",
    "    # Define calculation name depending on if there is a buffer or not\n",
    "    if buffer_size:\n",
    "        calc_name = f'{var}_{buffer_size}m'\n",
    "    else:\n",
    "        calc_name = var\n",
    "\n",
    "    # Check if the calculation has been done before (for multiple pollutants)\n",
    "    if calc_name in computed_variables.columns.tolist():\n",
    "        # If so, retrieve values\n",
    "        return computed_variables[['name',calc_name]]\n",
    "    # Otherwise, continue the calculation\n",
    "\n",
    "    # Initialize a df to store the calculaions\n",
    "    pt_calculations = station_info[['name']].copy()\n",
    "    # Create the list of point geometries to compute\n",
    "    input_points = [Point(coord) for coord in stations['location']]  \n",
    "\n",
    "    # Process raster data\n",
    "    if data_file.endswith('.tif'):\n",
    "        with rasterio.open(data_file) as src:\n",
    "            # Save important info of the raster\n",
    "            raster_info = {'array':src.read(1), 'resolution':src.res[0], 'xmin':src.bounds.left, 'ymax':src.bounds.top, 'nodata':src.nodata}\n",
    "            # Create a buffer mask\n",
    "            mask = create_mask(raster_info, buffer_size) \n",
    "            # Define the partial function pre-filled with arguments\n",
    "            func = partial(calculate_buffer_value, var=var, raster_info=raster_info, mask=mask)\n",
    "            # Save the values of all of the points calculated for the buffer value\n",
    "            pt_calculations[calc_name] = parallel_compute(func, input_points)\n",
    "    # Process vector data\n",
    "    else:\n",
    "        vector = gpd.read_file(data_file)\n",
    "        # Process bus stop buffer\n",
    "        if var == 'bus_stops':\n",
    "            # Create sample buffer geometry to use multiple times\n",
    "            buffer_template = Point(0,0).buffer(buffer_size, quad_segs=2)\n",
    "            # Define the partial function pre-filled with arguments\n",
    "            func = partial(calculate_vector_buffer, vector_data=vector, buffer_geom=buffer_template)\n",
    "            # Save the values of all of the points calculated for the buffer value\n",
    "            pt_calculations[calc_name] = parallel_compute(func, input_points)\n",
    "        # Otherwise calculate distance for remaining vector variables\n",
    "        else:\n",
    "            # Create STRtree spatial index from the gdf\n",
    "            geoms = vector.geometry.values\n",
    "            tree = STRtree(geoms)\n",
    "            # Set the name and parallel compute the distance calculations\n",
    "            func = partial(distance_calculations, tree=tree, geoms=geoms)\n",
    "            # Save calculated values\n",
    "            pt_calculations[calc_name] = parallel_compute(func, input_points)\n",
    "    \n",
    "    # Save values to computed_variables table\n",
    "    computed_variables = pd.merge(computed_variables, pt_calculations, on='name', how='left')\n",
    "    # Return the df of computed variables\n",
    "    return pt_calculations\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "\n",
    "# MAIN: Calculate optimized independant variable values\n",
    "def calc_indep_var(pollutant):\n",
    "    # Declare global var table\n",
    "    global indep_val_table\n",
    "    # Iterate over each independant variable\n",
    "    for indep_var in indep_vars:\n",
    "        init_buffers = [100,500,1000,2000,3000]\n",
    "        for buffer in init_buffers:\n",
    "            calc_name = f'{indep_var[\"var\"]}_{buffer}m'\n",
    "            # For variables with buffer calculations\n",
    "            if indep_var['buffer'] == 1:\n",
    "                calculate_points(station_info, indep_var, buffer)\n",
    "            # Non-buffer calculations\n",
    "            else:\n",
    "                calculate_points(station_info, indep_var, None) # Buffer size = None    # Don't need to return pt_calculations here because it should be saved in computed_variables\n",
    "\n",
    "# Initialize df to store all computed variables to prevent double calculation\n",
    "computed_variables = station_info.drop(['location'], axis=1)\n",
    "# Initialize df to store final optimized values\n",
    "indep_val_table = station_info['name'].copy()\n",
    "# Calculate variables for all pollutants and combine into one table\n",
    "for pollutant in pollutants:\n",
    "    indep_val_table = calc_indep_var(pollutant)\n",
    "\n",
    "# Save table to csv\n",
    "computed_variables.to_csv('x_values_5.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base independant variable maps\n",
    "# Creating for every buffer size found in the buffer optimizor flow for each pollutant\n",
    "    # Could technically do less and only include ones where coef != 0\n",
    "\n",
    "# Define array model\n",
    "Base = declarative_base()\n",
    "class BaseMaps(Base):\n",
    "    __tablename__ = 'base_maps'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    array_id = Column(String, index=True, nullable=False)\n",
    "    values = Column(ARRAY(Float))\n",
    "# Setup db to save the base maps\n",
    "def setup_base_db():\n",
    "    # Set-up db connection\n",
    "    engine = get_engine()\n",
    "    # Remove table if it already exists\n",
    "    inspector = inspect(engine)\n",
    "    if 'base_maps' in inspector.get_table_names():\n",
    "        BaseMaps.__table__.drop(engine)\n",
    "    # Create the table\n",
    "    Base.metadata.create_all(engine) \n",
    "    # Create a session for adding all of the base maps in one transaction\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    return engine, session\n",
    "\n",
    "# Create the input points (only the ones in the mask that I need)\n",
    "# TODO: might not actually be in the order I need oop\n",
    "def create_input_points():\n",
    "    # Read the raster info\n",
    "    with rasterio.open(f_boundary_r) as src:\n",
    "        raster_data = src.read(1)\n",
    "        # Get indices of non-NaN values\n",
    "        non_nan_indices = np.where(raster_data != 0)\n",
    "        rows = non_nan_indices[0]\n",
    "        cols = non_nan_indices[1]\n",
    "        # Get affine transform\n",
    "        transform = src.transform\n",
    "        # Get coordinates using vectorized operation\n",
    "        xs, ys = rasterio.transform.xy(transform, rows, cols)\n",
    "        # Convert to numpy arrays\n",
    "        x_coords = np.array(xs)\n",
    "        y_coords = np.array(ys)\n",
    "        # Create array of coordinate tuples\n",
    "        points_array = np.array([Point(x, y) for x, y in zip(x_coords, y_coords)])\n",
    "        print(f'{len(points_array)} points to be calculated in map\\n')\n",
    "    return points_array, non_nan_indices\n",
    "\n",
    "def store_array(array_id, values):\n",
    "    print(f'Saving {array_id} to sql db...')\n",
    "    base_map = BaseMaps(\n",
    "        array_id=array_id,\n",
    "        values=values)\n",
    "    session.add(base_map)\n",
    "    session.commit()\n",
    "\n",
    "# Function to seperate points into feasible amount of parallel computation chunks\n",
    "def chunker(input):\n",
    "    size = 1000\n",
    "    for pos in tqdm(range(0, len(input), size)):\n",
    "        yield input[pos : pos+size]\n",
    "\n",
    "# Main calculation function (altered version of calculate points)\n",
    "def static_mapper(input_points, input_calculation, buffer_size):\n",
    "    # Define variables from input calculation\n",
    "    var = input_calculation['var']\n",
    "    data_file = input_calculation['var_data']\n",
    "\n",
    "    # Process raster data\n",
    "    if data_file.endswith('.tif'):\n",
    "        with rasterio.open(data_file) as src:\n",
    "            # Save important info of the raster\n",
    "            raster_info = {'array':src.read(1), 'resolution':src.res[0], 'xmin':src.bounds.left, 'ymax':src.bounds.top, 'nodata':src.nodata}\n",
    "            # Create a buffer mask\n",
    "            mask = create_mask(raster_info, buffer_size) \n",
    "            # Define the partial function pre-filled with arguments\n",
    "            func = partial(calculate_buffer_value, var=var, raster_info=raster_info, mask=mask)\n",
    "            # Save the values of all of the points calculated for the buffer value\n",
    "            point_values = []\n",
    "            for chunk in chunker(input_points):\n",
    "                point_values.extend(parallel_compute(func, chunk))\n",
    "    # Process bus data\n",
    "    else:\n",
    "        vector = gpd.read_file(data_file)\n",
    "        # Create sample buffer geometry to use multiple times\n",
    "        buffer_template = Point(0,0).buffer(buffer_size, quad_segs=2)\n",
    "        # Define the partial function pre-filled with arguments\n",
    "        func = partial(calculate_vector_buffer, vector_data=vector, buffer_geom=buffer_template)\n",
    "        # Save the values of all of the points calculated for the buffer value\n",
    "        point_values = parallel_compute(func, input_points)\n",
    "    \n",
    "    # Return a list of values\n",
    "    return [float(point_val) for point_val in point_values]\n",
    "\n",
    "# Process the precalculated rasters to be in the right format and save to db\n",
    "# Should generally work bc they are the same size as the boundary raster, note that the rasters have misaligned extents for some reason though, could be a problem\n",
    "# TODO: The boundary vector creates a different calc so watch out\n",
    "\n",
    "def precalc_processor(precalc_vars, non_nan_indices):\n",
    "    # Collect and prepare non-nan indices\n",
    "    row_i, col_i = non_nan_indices\n",
    "    indices = np.stack((row_i, col_i), axis=1)\n",
    "    sorted_idx = np.lexsort((col_i, row_i))\n",
    "    sorted_indices = indices[sorted_idx]\n",
    "    # Iterate over each calculation\n",
    "    for var in precalc_vars:\n",
    "        with rasterio.open(var['var_data']) as src:\n",
    "            raster_data = src.read(1)\n",
    "            filtered_data = [float(raster_data[row,col]) for row,col in sorted_indices]\n",
    "            store_array(var['var'], filtered_data)\n",
    "\n",
    "indep_vars =[{'var':'lu_urban', 'var_data':f_landuse, 'buffer':1},\n",
    "              {'var':'lu_grass', 'var_data':f_landuse, 'buffer':1},\n",
    "              {'var':'lu_forest', 'var_data':f_landuse, 'buffer':1},\n",
    "              {'var':'build_cover', 'var_data':f_build_h, 'buffer':1},\n",
    "              {'var':'build_vol', 'var_data':f_build_h, 'buffer':1},\n",
    "              {'var':'build_std', 'var_data':f_build_h, 'buffer':1},\n",
    "              {'var':'ndvi', 'var_data':f_ndvi, 'buffer':1},\n",
    "              {'var':'elevation', 'var_data':f_elevation, 'buffer':1},\n",
    "              {'var':'pop', 'var_data':f_pop, 'buffer':1},\n",
    "              {'var':'mj_road_ln', 'var_data':f_major_rd_r, 'buffer':1},\n",
    "              {'var':'mi_road_ln', 'var_data':f_minor_rd, 'buffer':1},\n",
    "              {'var':'parking', 'var_data':f_parking, 'buffer':1},\n",
    "              {'var':'bus_stops', 'var_data':f_bus_stops, 'buffer':1},]\n",
    "precalc_vars = [{'var':'pow_plant', 'var_data':f_pplant_r, 'buffer':0},\n",
    "              {'var':'airport', 'var_data':f_airport_r, 'buffer':0},\n",
    "              {'var':'mj_road_dis', 'var_data':f_major_rd_prox, 'buffer':0}]\n",
    "\n",
    "# Set buffer sizes according to optimal size #TODO: will want to do this in a more integrated way, this is messy rn \n",
    "buffer_sizes = {'O3':[750,2500,175,625,750,2500,100,375,100,500,2500,250,1531],\n",
    "                'NO2':[250,250,593,2000,3000,100,2250,4000,375,546,875,250,562],\n",
    "                'SO2':[1500,1250,100,875,375,1750,100,4000,1500,250,1625,875,1796],\n",
    "                'PM10':[2750,4000,4000,1125,1250,625,875,3750,1250,625,4000,500,4000],\n",
    "                'PM2_5':[2250,4000,4000,1125,1250,625,750,625,1750,625,100,375,750]}\n",
    "\n",
    "points_array, non_nan_indices = create_input_points()\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Set-up the db to store the basemaps\n",
    "engine, session = setup_base_db()\n",
    "# Create points array from boundary raster\n",
    "points_array, non_nan_indices = create_input_points()\n",
    "\n",
    "# Add precalculated arrays after processing into 1d form\n",
    "precalc_processor(precalc_vars, non_nan_indices)\n",
    "\n",
    "# Calculate the buffer values for each variable + buffer size\n",
    "# Really not the best way to do this\n",
    "for pollutant, sizes in buffer_sizes.items():\n",
    "    for i, var in enumerate(indep_vars):\n",
    "        point_values = static_mapper(points_array, var, sizes[i])\n",
    "        # Save calculation to db\n",
    "        basemap_name = f'{pollutant}_{var[\"var\"]}'\n",
    "        store_array(basemap_name, point_values)\n",
    "print('Basemap calculation concluded')\n",
    "session.close()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Step 5: Collect Current Pollutant Measurements<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect current measurements from API\n",
    "# Filter / fill gaps and bad values\n",
    "# Update the sql table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Step 6: Prepare Training Sets for Current Model<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data selector to query the pollution training data and create the samples for the model\n",
    "    # Same (range of) wind dir?\n",
    "\n",
    "# Merge non-wind direction columns for each station\n",
    "# Merge wind direct columns according to wind \n",
    "\n",
    "# Average all values for each station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Step 7: Calculate Model Coefficients<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the calculated training set\n",
    "# Sign-constrained LASSO makes the most sense\n",
    "\n",
    "# Add coefficients to the db\n",
    "    # not necessary imo, but would be nice to save them so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sign contraints + indep var calculations\n",
    "\n",
    "indep_vars =[{'var':'lu_urban', 'var_data':f_landuse, 'buffer':1},\n",
    "              {'var':'lu_grass', 'var_data':f_landuse, 'buffer':1},\n",
    "              {'var':'lu_forest', 'var_data':f_landuse, 'buffer':1},\n",
    "              {'var':'build_cover', 'var_data':f_build_h, 'buffer':1},\n",
    "              {'var':'build_vol', 'var_data':f_build_h, 'buffer':1},\n",
    "              {'var':'build_std', 'var_data':f_build_h, 'buffer':1},\n",
    "              {'var':'ndvi', 'var_data':f_ndvi, 'buffer':1},\n",
    "              {'var':'elevation', 'var_data':f_elevation, 'buffer':1},\n",
    "              {'var':'pop', 'var_data':f_pop, 'buffer':1},\n",
    "              {'var':'mj_road_ln', 'var_data':f_major_rd_r, 'buffer':1},\n",
    "              {'var':'mi_road_ln', 'var_data':f_minor_rd, 'buffer':1},\n",
    "              {'var':'parking', 'var_data':f_parking, 'buffer':1},\n",
    "              {'var':'bus_stops', 'var_data':f_bus_stops, 'buffer':1},\n",
    "              {'var':'pow_plant', 'var_data':f_pplant_v, 'buffer':0},\n",
    "              {'var':'airport', 'var_data':f_airport_v, 'buffer':0},\n",
    "              {'var':'mj_road_dis', 'var_data':f_major_rd_v, 'buffer':0}\n",
    "           ]\n",
    "\n",
    "# Sign-constrained directions (for buffre calc)\n",
    "sign_constraints = {'O3':{'lu_urban':-1,\n",
    "                    'lu_grass':1,\n",
    "                    'lu_forest':1,\n",
    "                    'build_cover': 0,\n",
    "                    'build_vol':0,\n",
    "                    'build_std':0,\n",
    "                    'ndvi':1,\n",
    "                    'elevation':0,\n",
    "                    'pop':0,\n",
    "                    'mj_road_ln':-1,\n",
    "                    'mi_road_ln':-1,\n",
    "                    'parking':0,\n",
    "                    'bus_stops':0,\n",
    "                    'pow_plant':0,\n",
    "                    'airport':0,\n",
    "                    'mj_road_dis':0,\n",
    "                    'T':0,\n",
    "                    'RH':0,\n",
    "                    'pressure':0,\n",
    "                    'WS':-1},\n",
    "                'NO2':{'lu_urban':1,\n",
    "                    'lu_grass':-1,\n",
    "                    'lu_forest':-1,\n",
    "                    'build_cover': 1,\n",
    "                    'build_vol':1,\n",
    "                    'build_std':-1,\n",
    "                    'ndvi':-1,\n",
    "                    'elevation':-1,\n",
    "                    'pop':1,\n",
    "                    'mj_road_ln':1,\n",
    "                    'mi_road_ln':1,\n",
    "                    'parking':1,\n",
    "                    'bus_stops':1,\n",
    "                    'pow_plant':-1,\n",
    "                    'airport':-1,\n",
    "                    'mj_road_dis':-1,\n",
    "                    'T':-1,\n",
    "                    'RH':1,\n",
    "                    'pressure':1,\n",
    "                    'WS':-1},\n",
    "                'PM':{'lu_urban':1,\n",
    "                    'lu_grass':-1,\n",
    "                    'lu_forest':-1,\n",
    "                    'build_cover': 1,\n",
    "                    'build_vol':1,\n",
    "                    'build_std':-1,\n",
    "                    'ndvi':-1,\n",
    "                    'elevation':-1,\n",
    "                    'pop':1,\n",
    "                    'mj_road_ln':1,\n",
    "                    'mi_road_ln':1,\n",
    "                    'parking':1,\n",
    "                    'bus_stops':1,\n",
    "                    'pow_plant':-1,\n",
    "                    'airport':-1,\n",
    "                    'mj_road_dis':-1,\n",
    "                    'T':-1,\n",
    "                    'RH':-1,\n",
    "                    'pressure':0,\n",
    "                    'WS':-1},\n",
    "                'SO2':{'lu_urban':0,\n",
    "                    'lu_grass':-1,\n",
    "                    'lu_forest':-1,\n",
    "                    'build_cover': 1,\n",
    "                    'build_vol':1,\n",
    "                    'build_std':-1,\n",
    "                    'ndvi':-1,\n",
    "                    'elevation':0,\n",
    "                    'pop':1,\n",
    "                    'mj_road_ln':1,\n",
    "                    'mi_road_ln':1,\n",
    "                    'parking':1,\n",
    "                    'bus_stops':1,\n",
    "                    'pow_plant':-1,\n",
    "                    'airport':0,\n",
    "                    'mj_road_dis':-1,\n",
    "                    'T':-1,\n",
    "                    'RH':0,\n",
    "                    'pressure':0,\n",
    "                    'WS':-1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function that runs input data through model process for all pollutants\n",
    "def run_multi_pollutant_models(data, predictors, pollutants, sign_constraints, alphas=None, l1_ratios=None, cv=5, max_workers=None, verbose=True):\n",
    "    \n",
    "    # Setup default parameters if not provided\n",
    "    if alphas is None:\n",
    "        alphas = np.logspace(-3, 0, 4)  # [0.001, 0.01, 0.1, 1.0]\n",
    "    if l1_ratios is None:\n",
    "        l1_ratios = [0.1, 0.5, 0.9]\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {}\n",
    "    models = {}\n",
    "    \n",
    "    # Run model for each pollutant\n",
    "    for pollutant in pollutants:\n",
    "        if verbose:\n",
    "            print(f\"Modeling pollutant: {pollutant}\")\n",
    "        \n",
    "        # Prepare data for modelling\n",
    "        x_values = pd.read_csv(r'C:\\Users\\Austin\\Documents\\DATABANK\\Masters\\Thesis\\Code\\V5\\model_indep_vars2.csv')\n",
    "        # Filters the values that only have pollutant in their name\n",
    "        pol_x_vals = x_values[['name'] + x_values.filter(like=pollutant).columns.to_list()]\n",
    "        # Merges the pollutant data for each station onto the indep variables\n",
    "        training_data = data.merge(pol_x_vals, on='name', how='left')\n",
    "        # Filters data for pollutants again to get rid of name and extra measurement things like time\n",
    "        X = training_data[[col for col in training_data.columns if pollutant in col]]\n",
    "        X = X.drop(columns=[pollutant])\n",
    "        y = training_data[pollutant]\n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        l1_ratios = np.linspace(0.1, 1.0, 10)\n",
    "        alphas = np.logspace(-4, 1, 50)\n",
    "\n",
    "        model = ElasticNetCV(\n",
    "            l1_ratio=l1_ratios,\n",
    "            alphas=alphas,\n",
    "            cv=5,\n",
    "            random_state=42,\n",
    "            max_iter=10000,\n",
    "            tol=1e-4\n",
    "        )\n",
    "        model.fit(X_scaled, y)\n",
    "        \n",
    "        # Store results\n",
    "        results[pollutant] = {\n",
    "            'intercept': model.intercept_,\n",
    "            **{pred: coef for pred, coef in zip(predictors, model.coef_)},\n",
    "            'alpha': model.alpha_,\n",
    "            'l1_ratio': model.l1_ratio_\n",
    "        }\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "    \n",
    "    # Reorder columns to have intercept first, then predictors, then metrics\n",
    "    ordered_cols = ['intercept'] + predictors\n",
    "    results_df = results_df[ordered_cols]\n",
    "    \n",
    "    return results_df, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating model coefs and setting up SQLAlchemy engine\n",
    "\n",
    "# Function to return a new SQLAlchemy engine with appropriate connection pool settings\n",
    "# Replacement for API\n",
    "def get_engine():\n",
    "    return create_engine(\n",
    "        \"postgresql://thesis:thesis@localhost:5432/hist_data\",\n",
    "        pool_pre_ping=True,     # Verify connections before using them\n",
    "        pool_recycle=3600,      # Recycle connections after an hour\n",
    "        pool_size=5,            # Limit pool size\n",
    "        max_overflow=10         # Allow extra connections if needed\n",
    "    )\n",
    "\n",
    "# Using input time, query the database for historical data\n",
    "def create_training_set(date_string):\n",
    "    date_obj = datetime.strptime(date_string, '%Y-%m-%d %H:%M')\n",
    "    reformatted_string = date_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    # Connect to database\n",
    "    engine = get_engine()\n",
    "    # First collect current data\n",
    "    # Will be an API but I'm going to use the postgresql db\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM sensor_data\n",
    "    WHERE time = '{reformatted_string}'\n",
    "    \"\"\"\n",
    "    current_data = pd.read_sql_query(text(query), engine)\n",
    "    return current_data\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "# 20 predictors\n",
    "predictors = [\n",
    "    'lu_urban', 'lu_grass', 'lu_forest', 'build_cover', 'build_vol',\n",
    "    'build_std', 'ndvi', 'elevation', 'pop', 'mj_road_ln',\n",
    "    'mi_road_ln', 'parking', 'bus_stops', 'pow_plant', 'airport',\n",
    "    'mj_road_dis', 'T', 'RH', 'pressure', 'WS'\n",
    "]\n",
    "# 17 predictors\n",
    "predictors = [\n",
    "    'lu_urban', 'lu_grass', 'lu_forest', 'build_cover', 'build_vol',\n",
    "    'build_std', 'ndvi', 'elevation', 'pop', 'mj_road_ln',\n",
    "    'mi_road_ln', 'parking', 'bus_stops', 'pow_plant', 'airport',\n",
    "    'mj_road_dis'\n",
    "]\n",
    "# predictors with buffer size (so that the stored coefs can be used in mapping function)\n",
    "\n",
    "\n",
    "# Generate pollutant data with different relationships to predictors\n",
    "pollutants = ['O3', 'NO2', 'SO2', 'PM10', 'PM2_5']\n",
    "# Defined sign constraints ahead with buffer calculation\n",
    "sign_constraints = {'O3':{'lu_urban':-1,\n",
    "                    'lu_grass':1,\n",
    "                    'lu_forest':1,\n",
    "                    'build_cover': 0,\n",
    "                    'build_vol':0,\n",
    "                    'build_std':0,\n",
    "                    'ndvi':1,\n",
    "                    'elevation':0,\n",
    "                    'pop':0,\n",
    "                    'mj_road_ln':-1,\n",
    "                    'mi_road_ln':-1,\n",
    "                    'parking':0,\n",
    "                    'bus_stops':0,\n",
    "                    'pow_plant':0,\n",
    "                    'airport':0,\n",
    "                    'mj_road_dis':0,\n",
    "                    'T':0,\n",
    "                    'RH':0,\n",
    "                    'pressure':0,\n",
    "                    'WS':-1},\n",
    "                'NO2':{'lu_urban':1,\n",
    "                    'lu_grass':-1,\n",
    "                    'lu_forest':-1,\n",
    "                    'build_cover': 1,\n",
    "                    'build_vol':1,\n",
    "                    'build_std':-1,\n",
    "                    'ndvi':-1,\n",
    "                    'elevation':-1,\n",
    "                    'pop':1,\n",
    "                    'mj_road_ln':1,\n",
    "                    'mi_road_ln':1,\n",
    "                    'parking':1,\n",
    "                    'bus_stops':1,\n",
    "                    'pow_plant':-1,\n",
    "                    'airport':-1,\n",
    "                    'mj_road_dis':-1,\n",
    "                    'T':-1,\n",
    "                    'RH':1,\n",
    "                    'pressure':1,\n",
    "                    'WS':-1},\n",
    "                'PM':{'lu_urban':1,\n",
    "                    'lu_grass':-1,\n",
    "                    'lu_forest':-1,\n",
    "                    'build_cover': 1,\n",
    "                    'build_vol':1,\n",
    "                    'build_std':-1,\n",
    "                    'ndvi':-1,\n",
    "                    'elevation':-1,\n",
    "                    'pop':1,\n",
    "                    'mj_road_ln':1,\n",
    "                    'mi_road_ln':1,\n",
    "                    'parking':1,\n",
    "                    'bus_stops':1,\n",
    "                    'pow_plant':-1,\n",
    "                    'airport':-1,\n",
    "                    'mj_road_dis':-1,\n",
    "                    'T':-1,\n",
    "                    'RH':-1,\n",
    "                    'pressure':0,\n",
    "                    'WS':-1},\n",
    "                'SO2':{'lu_urban':0,\n",
    "                    'lu_grass':-1,\n",
    "                    'lu_forest':-1,\n",
    "                    'build_cover': 1,\n",
    "                    'build_vol':1,\n",
    "                    'build_std':-1,\n",
    "                    'ndvi':-1,\n",
    "                    'elevation':0,\n",
    "                    'pop':1,\n",
    "                    'mj_road_ln':1,\n",
    "                    'mi_road_ln':1,\n",
    "                    'parking':1,\n",
    "                    'bus_stops':1,\n",
    "                    'pow_plant':-1,\n",
    "                    'airport':0,\n",
    "                    'mj_road_dis':-1,\n",
    "                    'T':-1,\n",
    "                    'RH':0,\n",
    "                    'pressure':0,\n",
    "                    'WS':-1,}}\n",
    "\n",
    "# Function to run to turn a time into model coefficients\n",
    "# Depending on how I want to initiate model calculation, might change from being a timestamp. Currently good for demo\n",
    "def calc_model_coefs(input_datetime):\n",
    "    # Create the training set\n",
    "    current_data = create_training_set(input_datetime)\n",
    "    print(f'Modelling for {input_datetime}')\n",
    "\n",
    "    if current_data.shape[0] < 10:\n",
    "        print('Insufficient station data')\n",
    "        return None\n",
    "    else:\n",
    "        print(f'Queried training set, {current_data.shape[0]} samples')\n",
    "\n",
    "    # Run models for all pollutants\n",
    "    model_coefs, _ = run_multi_pollutant_models(\n",
    "        data=current_data, # Also each model has its own training data (can make data a dict with the training data inside) (not optimally efficient though, have this function include a filter_by_pollutant(copy of filter by deviation)\n",
    "        predictors=predictors, # Will have to fix this in the function to include the buffers (maybe)\n",
    "        pollutants=pollutants,\n",
    "        sign_constraints=sign_constraints,\n",
    "        alphas=np.logspace(-3, 0, 4),  # [0.001, 0.01, 0.1, 1.0]\n",
    "        l1_ratios = [0.1, 0.5, 0.9],\n",
    "        cv=5,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Flatten coefficients for all the pollutants into one dictionary\n",
    "    row_idcs = model_coefs.index.astype(str)\n",
    "    col_names = model_coefs.columns\n",
    "    m_flat = {'time':input_datetime}\n",
    "    for row_idx in row_idcs:\n",
    "        for col_name in col_names:\n",
    "            new_col_name = f'{row_idx}_{col_name}'\n",
    "            value = model_coefs.loc[row_idx, col_name]\n",
    "            m_flat[new_col_name] = value\n",
    "    return m_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and store n days of model coefficients in db (for now just do the last 24 hours)\n",
    "\n",
    "# Create the table\n",
    "def create_coef_table(sample_dict):\n",
    "    table_name = 'coef_data'\n",
    "    # Create engine\n",
    "    engine = create_engine('postgresql://thesis:thesis@localhost:5432/hist_data')\n",
    "    # Drop the table if it exists already\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS coef_data CASCADE\"))\n",
    "        conn.commit()\n",
    "    # Create tables using the modern pattern\n",
    "    Base = declarative_base()\n",
    "    # Define column types mapping\n",
    "    dtype_mapping = {\n",
    "        'int64': Integer,\n",
    "        'float64': Float,\n",
    "        'object': String,\n",
    "        'datetime64[ns]': DateTime,\n",
    "    }\n",
    "    # Create attributes dictionary\n",
    "    attrs = {\n",
    "        '__tablename__': table_name,\n",
    "        'id': Column(Integer, primary_key=True) # Primary key indicates that its an id maker\n",
    "    }\n",
    "    # Add each column from the sample dictionary\n",
    "    for column, value in sample_dict.items():\n",
    "        # Additional handling for NumPy types\n",
    "        value_type = type(value).__name__\n",
    "        \n",
    "        if 'float' in value_type:\n",
    "            sql_type = Float\n",
    "        elif 'int' in value_type:\n",
    "            sql_type = Integer\n",
    "        elif 'str' in value_type or 'object' in value_type:\n",
    "            sql_type = String\n",
    "        elif 'datetime' in value_type or 'Timestamp' in value_type:\n",
    "            sql_type = DateTime\n",
    "        else:\n",
    "            # Default to standard Python type mapping or String as fallback\n",
    "            sql_type = dtype_mapping.get(type(value), String)\n",
    "\n",
    "        attrs[column] = Column(sql_type)\n",
    "    # Create the model class\n",
    "    TableClass = type(table_name.capitalize(), (Base,), attrs)\n",
    "    # Create table\n",
    "    Base.metadata.create_all(engine)\n",
    "    return TableClass\n",
    "\n",
    "# Fill table with data / add new rows\n",
    "def populate_table(TableClass, coefs):\n",
    "    # Create engine\n",
    "    engine = create_engine('postgresql://thesis:thesis@localhost:5432/hist_data')\n",
    "\n",
    "    # Populate the table with data\n",
    "    with Session(engine) as session:\n",
    "        # Initialize list for converted python native types (not necessary if I format it correctly in the beginning)\n",
    "        processed_records = []\n",
    "        for record in coefs:\n",
    "            processed_record = {}\n",
    "            for key, value in record.items():\n",
    "                if 'numpy' in str(type(value)):\n",
    "                    if 'float' in str(type(value)):\n",
    "                        processed_record[key] = float(value)\n",
    "                    elif 'int' in str(type(value)):\n",
    "                        processed_record[key] = int(value)\n",
    "                    elif 'datetime' in str(type(value)) or 'Timestamp' in str(type(value)):\n",
    "                        processed_record[key] = value.to_pydatetime() if hasattr(value, 'to_pydatetime') else value\n",
    "                    else:\n",
    "                        processed_record[key] = str(value)\n",
    "                else:\n",
    "                    processed_record[key] = value\n",
    "            processed_records.append(processed_record)\n",
    "        \n",
    "        for record in processed_records:\n",
    "            # Create a new instance of the model with the record data\n",
    "            model_instance = TableClass(**record)\n",
    "            session.add(model_instance)\n",
    "        \n",
    "        # Commit the session to save all records\n",
    "        session.commit()\n",
    "        print(f\"Added {len(coefs)} records to {TableClass.__tablename__} table\")\n",
    "\n",
    "input_date = '2024-12-04 22:00'\n",
    "prev_hours = 6\n",
    "# Generate strings for the last 24 hours in a list\n",
    "base_date = datetime.strptime(input_date, '%Y-%m-%d %H:%M')\n",
    "input_dates = []\n",
    "for i in range(prev_hours):\n",
    "    new_dt = base_date - timedelta(hours=i)\n",
    "    new_dt_format = new_dt.strftime('%Y-%m-%d %H:%M')\n",
    "    input_dates.append(new_dt_format)\n",
    "\n",
    "# Create the table using a sample (input date)\n",
    "sample_dict = calc_model_coefs(input_date) # TODO: in this process, you calculate the first date twice, not really necessary\n",
    "coef_table = create_coef_table(sample_dict)\n",
    "\n",
    "# Iterate over each date, calculate model coefficients, and add to the db\n",
    "coefs_ls = []\n",
    "for date in input_dates:\n",
    "    model_coefs = calc_model_coefs(date)\n",
    "    if model_coefs: # Checking if there is data or not\n",
    "        coefs_ls.append(model_coefs)\n",
    "populate_table(coef_table, coefs_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Step 8: Regression Prediction Mapping<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use current wind direction data to calculate 100m wind field (3 bit data storage?)\n",
    "# Create wind-direction base maps with wind field + indep var base maps\n",
    "# Use coefs + intercept to calculate the prediction map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use model coefficients to calculate the regression prediction maps\n",
    "# Prediction maps are going to be saved in memory for now\n",
    "\n",
    "def pollutant_preds(pollutant, timestamp):\n",
    "    print(f'Building Prediction Map for {pollutant} at {timestamp}...')\n",
    "    \n",
    "    # Set up db connection:\n",
    "    engine = get_engine()\n",
    "    # Query coefs for the timestamp\n",
    "    query = f\"SELECT * FROM coef_data WHERE time = '{timestamp}'\"\n",
    "    coefs_df = pd.read_sql_query(text(query), engine)\n",
    "    # Filter the coefs df to only include pollutant values\n",
    "    pol_coefs = coefs_df.loc[:,coefs_df.columns.str.contains(pollutant)]\n",
    "    # Filter coefficients that equal zero\n",
    "    filter_coefs = pol_coefs.loc[:,pol_coefs.iloc[0] != 0]\n",
    "\n",
    "    # Get scaling params to rescale maps (could pickle the scaler but won't for now)\n",
    "    x_values = pd.read_csv(r'C:\\Users\\Austin\\Documents\\DATABANK\\Masters\\Thesis\\Code\\V5\\model_indep_vars2.csv')\n",
    "    filtered_cols = [col1 for col1 in x_values.columns if any(col2 in col1 for col2 in filter_coefs)]\n",
    "    \n",
    "    if len(filtered_cols) < 4:\n",
    "        print('Insufficient predictors')\n",
    "        return None\n",
    "    else:\n",
    "        print(f'Independant Variables Used in Map Creation: {filtered_cols}')\n",
    "\n",
    "    filtered_x_vals = x_values[filtered_cols]\n",
    "    scaler = StandardScaler()\n",
    "    _ = scaler.fit_transform(filtered_x_vals)\n",
    "    # Get scaling parameters for x values\n",
    "    feature_means = scaler.mean_\n",
    "    feature_stds = scaler.scale_\n",
    "    # Dictionary to map feature names to their scaling params\n",
    "    scaling_params = {}\n",
    "    for i, feature in enumerate(filtered_x_vals.columns):\n",
    "        scaling_params[feature] = {\n",
    "            'mean':feature_means[i],\n",
    "            'std':feature_stds[i]}\n",
    "    \n",
    "    # Apply scaling params to maps\n",
    "    # Iterate over each basemap / x value\n",
    "    for variable in filtered_x_vals.columns:\n",
    "        dist_var = False\n",
    "        \n",
    "        # Need to reformat the variable for query so that it doesn't contain pollutant for distance calcs or buffer size for buffer calcs\n",
    "        # TODO: Really need to clean up this code and make PM2_5 PM25 instead or its so confusing\n",
    "        if any(precalc in variable for precalc in precalcs):\n",
    "            dist_var = True\n",
    "            # Remove the pollutant before\n",
    "            if pollutant == 'PM2_5': # Have to do seperate one bc I'm a dummy\n",
    "                q_variable = variable.split('_',2)[2]\n",
    "            else:\n",
    "                q_variable = variable.split('_',1)[1]\n",
    "        else:\n",
    "            # Remove meters after for non-distance variables\n",
    "            q_variable = variable.rsplit('_',1)[0]\n",
    "        \n",
    "        query = f\"SELECT values FROM base_maps WHERE array_id = '{q_variable}'\"\n",
    "        df = pd.read_sql_query(query, engine)\n",
    "        array = np.array(df['values'].iloc[0])\n",
    "\n",
    "        # Multiply precalcs by 100 TODO: need to redo this when I save the maps to postgres\n",
    "        if any(precalc in variable for precalc in precalcs):\n",
    "            array = array * 100\n",
    "\n",
    "        # Scale array\n",
    "        scaled_array = (array - scaling_params[variable]['mean']) / scaling_params[variable]['std']\n",
    "        # Multiply coefficient\n",
    "        # Horrible way to do this but I need to set a condition for the distance calcs\n",
    "        if dist_var:\n",
    "            contribution_array = scaled_array * filter_coefs[variable][0]\n",
    "        else:\n",
    "            contribution_array = scaled_array * filter_coefs[q_variable][0]\n",
    "        # Add to prediction array\n",
    "        if 'prediction_array' not in locals():\n",
    "            prediction_array = contribution_array\n",
    "        else:\n",
    "            prediction_array += contribution_array\n",
    "    # Finally add intercept\n",
    "    intercept_val = filter_coefs[f'{pollutant}_intercept'][0]\n",
    "    print(f'Adding intercept: {intercept_val}')\n",
    "    prediction_array += intercept_val\n",
    "    return prediction_array\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "# Calculate prediction\n",
    "input_time = '2024-12-04 22:00'\n",
    "# Set PM rolling average count\n",
    "rolling_avg_hours = 6\n",
    "precalcs = ['pow_plant', 'airport', 'mj_road_dis']\n",
    "\n",
    "predictions = {}\n",
    "pollutants = ['O3','NO2','SO2','PM10','PM2_5']\n",
    "for pollutant in pollutants:\n",
    "    \n",
    "    if pollutant in ['PM10', 'PM2_5']:\n",
    "        # Create list of timestamps for rolling average calculation\n",
    "        timestamps = []\n",
    "        dt_ts = datetime.strptime(input_time, '%Y-%m-%d %H:%M')\n",
    "        for i in range(rolling_avg_hours):\n",
    "            new_dt = dt_ts - timedelta(hours=i)\n",
    "            new_dt_format = new_dt.strftime('%Y-%m-%d %H:%M')\n",
    "            timestamps.append(new_dt_format)\n",
    "        # Initialize list of arrays to store all of the past values\n",
    "        past_arrays = []\n",
    "        for timestamp in timestamps:\n",
    "            past_arr = pollutant_preds(pollutant, timestamp)\n",
    "            # Don't add the array if the model 'fails'\n",
    "            if past_arr is not None:\n",
    "                past_arrays.append(past_arr)\n",
    "            else:\n",
    "                continue\n",
    "        # Stack arrays into 2d and average\n",
    "        if len(past_arr) > 1:\n",
    "            stacked = np.vstack(past_arrays)\n",
    "            predictions[pollutant] = np.mean(stacked, axis=0)\n",
    "        else:\n",
    "            predictions[pollutant] = None\n",
    "    # Normal calculation for other variables\n",
    "    else:\n",
    "        predictions[pollutant] = pollutant_preds(pollutant, input_time)\n",
    "        # Will save none value for model failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Step 9: Regression Kriging<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set station information in map\n",
    "\n",
    "# Creates dictionaries of the indices in both 1d and 2d arrays for each station\n",
    "def create_closest_point_dict_vectorized(df, raster_path):\n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        raster_data = src.read(1)\n",
    "        transform = src.transform\n",
    "        \n",
    "        # Get indices of non-NaN values (valid points)\n",
    "        non_nan_indices = np.where(raster_data != 0)\n",
    "        valid_rows, valid_cols = non_nan_indices\n",
    "        \n",
    "        # Create arrays to store row,col for each location in df\n",
    "        df_rows = np.zeros(len(df), dtype=int)\n",
    "        df_cols = np.zeros(len(df), dtype=int)\n",
    "        \n",
    "        # Convert all df locations to row,col in one go\n",
    "        for i, (x, y) in enumerate(df['location']):\n",
    "            r, c = rowcol(transform, x, y)\n",
    "            df_rows[i] = r\n",
    "            df_cols[i] = c\n",
    "        \n",
    "        # Create result dictionaries\n",
    "        closest_point_dict = {}  # 1D flattened index\n",
    "        indices_2d_dict = {}     # 2D array indices (row, col)\n",
    "        \n",
    "        # Process each df point\n",
    "        for i, name in enumerate(df['name']):\n",
    "            r, c = df_rows[i], df_cols[i]\n",
    "            \n",
    "            # Check if this point is valid in the raster\n",
    "            if 0 <= r < raster_data.shape[0] and 0 <= c < raster_data.shape[1] and raster_data[r, c] != 0:\n",
    "                # Find position in the non_nan_indices\n",
    "                flat_idx = np.where((valid_rows == r) & (valid_cols == c))[0][0]\n",
    "                closest_r, closest_c = r, c\n",
    "            else:\n",
    "                # Calculate distances to all valid points\n",
    "                distances = np.sqrt((valid_rows - r)**2 + (valid_cols - c)**2)\n",
    "                # Find the closest valid point\n",
    "                min_idx = np.argmin(distances)\n",
    "                flat_idx = min_idx\n",
    "                closest_r, closest_c = valid_rows[min_idx], valid_cols[min_idx]\n",
    "            \n",
    "            closest_point_dict[name] = flat_idx\n",
    "            indices_2d_dict[name] = (closest_r, closest_c)\n",
    "            \n",
    "        return closest_point_dict, indices_2d_dict\n",
    "\n",
    "station_locs_1d, station_locs_2d = create_closest_point_dict_vectorized(station_info, f_boundary_r)\n",
    "print(station_locs_1d)\n",
    "print(len(station_locs_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with kriging variables\n",
    "\n",
    "current_measures = create_training_set(input_time)\n",
    "pollutant = 'SO2'\n",
    "current_pol_vals = {name: pol for name, pol in zip(current_measures['name'], current_measures[pollutant])}\n",
    "print(current_pol_vals)\n",
    "\n",
    "def save_raster(map, output_path):\n",
    "    # Open the boundary raster as a template\n",
    "    with rasterio.open(f_boundary_r) as src:\n",
    "        profile = src.profile.copy()\n",
    "        raster_shape = src.shape\n",
    "        # Create empty raster of zeroes\n",
    "        output_raster = np.zeros(raster_shape, dtype=map.dtype)\n",
    "        # Get row and col indices\n",
    "        rows = non_nan_indices[0]\n",
    "        cols = non_nan_indices[1]\n",
    "        # Place 1d values back into 2d raster\n",
    "        output_raster[rows,cols] = map\n",
    "\n",
    "        # Write the new raster to the output path\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(output_raster, 1)\n",
    "\n",
    "def krig(values):\n",
    "    # TODO: define all of this stuff beforehand, doesn't need to be done multiple times\n",
    "    # Define the coordinates (indexes) of the stations\n",
    "    names = list(station_info['name'])\n",
    "    y_coords = np.array([float(station_locs_2d[name][0]) for name in names])\n",
    "    x_coords = np.array([float(station_locs_2d[name][1]) for name in names])\n",
    "\n",
    "    values_arr = np.array([values[name] for name in names])\n",
    "    # Define the grid over which to do interpolation\n",
    "    grid_x = np.arange(188, dtype=float)\n",
    "    grid_y = np.arange(177, dtype=float)\n",
    "\n",
    "    OK = OrdinaryKriging(\n",
    "        x_coords, y_coords, values_arr,\n",
    "        variogram_model='linear',\n",
    "        variogram_parameters={'slope': 1.0, 'nugget': 0.1},\n",
    "        verbose = False,\n",
    "        enable_plotting=True,\n",
    "        nlags=10\n",
    "    )\n",
    "    # Krige the grid with the data values\n",
    "    z, ss = OK.execute('grid', grid_x, grid_y)\n",
    "    # Save krig into 1d array within boundary\n",
    "    rows = non_nan_indices[0]\n",
    "    cols = non_nan_indices[1]\n",
    "    oneD_krig = z[rows,cols]\n",
    "    return np.array(oneD_krig)\n",
    "\n",
    "so2_krig = krig(current_pol_vals)\n",
    "save_raster(so2_krig, r'rasters\\so2_krig3.tif')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Create RK postgres table\n",
    "# Can make complicated timescaledb stuff later since I don't even have enough for it to matter\n",
    "def create_output_table()\n",
    "    \n",
    "def save_map():\n",
    "'''\n",
    "\n",
    "def save_raster(map, output_path):\n",
    "    # Open the boundary raster as a template\n",
    "    with rasterio.open(f_boundary_r) as src:\n",
    "        profile = src.profile.copy()\n",
    "        raster_shape = src.shape\n",
    "        # Create empty raster of zeroes\n",
    "        output_raster = np.zeros(raster_shape, dtype=map.dtype)\n",
    "        # Get row and col indices\n",
    "        rows = non_nan_indices[0]\n",
    "        cols = non_nan_indices[1]\n",
    "        # Place 1d values back into 2d raster\n",
    "        output_raster[rows,cols] = map\n",
    "\n",
    "        # Write the new raster to the output path\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(output_raster, 1)\n",
    "\n",
    "# Function to krig station points and return 1d array\n",
    "# Values have to be in the right order! \n",
    "def krig(values):\n",
    "    # TODO: define all of this stuff beforehand, doesn't need to be done multiple times\n",
    "    # Define the coordinates (indexes) of the stations\n",
    "    names = list(station_info['name'])\n",
    "    y_coords = np.array([float(station_locs_2d[name][0]) for name in names])\n",
    "    x_coords = np.array([float(station_locs_2d[name][1]) for name in names])\n",
    "\n",
    "    values_arr = np.array([values[name] for name in names])\n",
    "    # Define the grid over which to do interpolation\n",
    "    grid_x = np.arange(188, dtype=float)\n",
    "    grid_y = np.arange(177, dtype=float)\n",
    "\n",
    "    OK = OrdinaryKriging(\n",
    "        x_coords, y_coords, values_arr,\n",
    "        variogram_model='linear',\n",
    "        verbose = False,\n",
    "        enable_plotting=False\n",
    "    )\n",
    "    # Krige the grid with the data values\n",
    "    z, ss = OK.execute('grid', grid_x, grid_y)\n",
    "    \n",
    "    # Save krig into 1d array within boundary\n",
    "    rows = non_nan_indices[0]\n",
    "    cols = non_nan_indices[1]\n",
    "    oneD_krig = z[rows,cols]\n",
    "    return np.array(oneD_krig)\n",
    "\n",
    "def regkrig(values, prediction): \n",
    "    names = list(station_info['name'])\n",
    "    # Calculate predicted values at each station from the maps\n",
    "    station_predictions = np.array([prediction[station_locs_1d[station]] for station in names])\n",
    "    values_arr = np.array([values[name] for name in names]) #TODO: Not calculating average value for PM in this\n",
    "    # Calculate residuals\n",
    "    residuals = values_arr - station_predictions\n",
    "    residual_dict = {name:residual for name,residual in zip(names,residuals)}\n",
    "    # Krig residuals\n",
    "    res_krig = krig(residual_dict)\n",
    "    # save residual krig for inspection\n",
    "    save_raster(res_krig, rf'rasters\\{}')\n",
    "    \n",
    "    # Add back to the prediction\n",
    "    return prediction + res_krig\n",
    "\n",
    "\n",
    "# Discrete and continuous to visualize\n",
    "# Just do continuous for now\n",
    "aqi_levels = [1,2,3,4,5,6]\n",
    "breakpoints = [\n",
    "[0,50,100,130,240,380], #O3\n",
    "[0,40,90,120,230,340], #NO2\n",
    "[0,100,200,350,500,750], #SO2\n",
    "[0,20,40,50,100,150], #PM10\n",
    "[0,10,20,25,50,75]  #PM2.5\n",
    "]\n",
    "\n",
    "# Needs a bit more documentation ngl\n",
    "def calc_aqi(pollutant_arrays):\n",
    "    def aqi_bin(pollutant_arr, pol_breakpoint):\n",
    "        return np.interp(pollutant_arr, pol_breakpoint, aqi_levels)\n",
    "    all_pollutants = np.stack(list(pollutant_arrays.values()), axis=0)\n",
    "    aqi_maps = np.array([aqi_bin(all_pollutants[i], breakpoints[i]) for i in range(len(breakpoints))])\n",
    "    aqi_array = np.max(aqi_maps, axis=0)\n",
    "    return aqi_array\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "\n",
    "# Get current measurements using previous function\n",
    "# Should make a new function that also properly formats PM values for kriging (just not going to bother with it now)\n",
    "current_measures = create_training_set(input_time)\n",
    "\n",
    "pollutant_maps = {}\n",
    "# Calculate map for each pollutant\n",
    "for pollutant in pollutants:\n",
    "    print(f'Calculating final map for {pollutant}')\n",
    "    # Get pollutant values\n",
    "    # TODO: Need to set up stronger contingency for when not all of the measurements are there\n",
    "\n",
    "    current_pol_vals = {name: pol for name, pol in zip(current_measures['name'], current_measures[pollutant])}\n",
    "    # Get prediction from previous step\n",
    "    prediction = predictions[pollutant]\n",
    "    \n",
    "    # If the model failed\n",
    "    # For now, the reason is it regularized too many variables, later I may just test if its better or not (if calculating the OK everytime, may need to move krig)\n",
    "    if prediction is None:\n",
    "        print('No prediction, kriging values')\n",
    "        map = krig(current_pol_vals)\n",
    "    else:\n",
    "        print('Regression kriging')\n",
    "        map = regkrig(current_pol_vals, prediction)\n",
    "\n",
    "    # Save map \n",
    "    #save_map(map)\n",
    "    output_path = rf'rasters\\rk_{pollutant}.tif'\n",
    "    save_raster(map, output_path)\n",
    "    # Save to dictionary for AQI calc\n",
    "    pollutant_maps[pollutant] = map\n",
    "\n",
    "# Calculate AQI map from all of the arrays\n",
    "#aqi_map = calc_aqi(pollutant_maps)\n",
    "\n",
    "# Save AQI to table\n",
    "output_path = rf'rasters\\rk_aqi.tif'\n",
    "save_raster(aqi_map, output_path)\n",
    "#save_map(aqi_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Step 10: Validation / Mapping Confidence<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LOOCV accuracy of current hour\n",
    "# Go through each station\n",
    "    # Use training set from step 6 saved in memory to take out the station\n",
    "        # technically would want to re-query removing the station from the avg calc used in the data selector\n",
    "    # Calculate coefs based on this\n",
    "    # Predict station points (not the removed station)\n",
    "    # Krig residuals but only calculating the removed station\n",
    "# Calculate MAE / r2 for all the predicted and actual values combined into one set\n",
    "\n",
    "# Will be adapted to do every hour for final validation assessment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
